{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MxTLZtnnC6qS"
      },
      "source": [
        "<center> <h3>Sentiment Analysis on Stock Tweets</h3> </center>\n",
        "<center><h4>Sina Soltanieh, Byron Phan, Nadezhda Shiroglazova</h4></center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ED0UU7C6qT"
      },
      "source": [
        "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YoK8fBccC6qT"
      },
      "source": [
        "#### Executive Summary:\n",
        "\n",
        "This project is focused on predicting the sentiment of tweets regarding stock performance, which has been spurred by increased social media presence and volatility of prices since the COVID-19 pandemic began. Our data consists of two datasets, one for tweets which our used in our machine learning model, and one for public stock financials, which we wrangle prepare for future machine learning work. We are interested in classifying these tweets into three classes: positive, negative, and neutral. To classify the sentiment of the tweets, we try three machine learning models: Linear SVM, Logistic Regression, and Multinomial Naive Bayes. We find that Logistic Regression and Linear SVM are best suited for this application, with Linear SVM slightly favored when selecting the best 100 features. Our results demonstrate that sentiment of stock tweets can be classified by these models, albeit with significant room for improvement."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zEAUjJFzC6qT"
      },
      "source": [
        "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c16CdEBrC6qU"
      },
      "source": [
        "## Outline\n",
        "1. <a href='#1'>INTRODUCTION</a>\n",
        "2. <a href='#2'>METHOD</a>\n",
        "3. <a href='#3'>RESULTS</a>\n",
        "4. <a href='#4'>DISCUSSION</a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MVYgTJzHC6qU"
      },
      "source": [
        "<a id=\"1\"></a>\n",
        "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PeeHaN4AC6qV"
      },
      "source": [
        "## 1. INTRODUCTION"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dT4NqF3JC6qW"
      },
      "source": [
        "<h4>Problem Statement</h4>\n",
        "\n",
        "This project is focused on predicting the sentiment of tweets regarding stock performance. We believe that the ability to analyze the sentiment of stock tweets can help predict the future performance of the underlying securities. By also analyzing financial data it may even be possible to predict a target closing price.\n",
        "\n",
        "This project only covers the analysis of stock tweets with machine learning techniques but we do also demonstrate some preliminary steps to preparing financial data for ML analysis.\n",
        "\n",
        "<h4>Significance of the Problem</h4>\n",
        "\n",
        "The rise of “meme stocks” such as Gamestop, which gained huge social media traction and astronomical gains and losses within mere hours, begs the question of the degree of influence social media has on stock prices. Thus, this project will focus on collecting and analyzing tweets about public stocks and use a predictive model to estimate the closing price of these stocks on future days. \n",
        "\n",
        "Previous work has been done in this field. Kordonis et al. analyzed how stock prices are effected by tweet sentiment. They used ML algorithms including SVM and Naive Bayes and achieved an accuracy of 87% with closing price prediction errors under 10%. \n",
        "\n",
        "Kordonis, J., Symeonidis, S., & Arampatzis, A. (2016). Stock Price Forecasting via Sentiment Analysis on Twitter. In Proceedings of the 20th Pan-Hellenic Conference on Informatics. PCI ’16: 20th Pan-Hellenic Conference on Informatics. ACM. https://doi.org/10.1145/3003733.3003787\n",
        "\n",
        "<h4>Questions</h4>\n",
        "\n",
        "* What ML model would we use to minimize the classifier accuracy difference between testing and training data to increase generalization of the model?\n",
        "* Are there any financial data that correlate with each other (e.g. High might correlate with HP%)?\n",
        "* How can we improve the performance of sentiment classification specifically for tweets?\n",
        "\n",
        "These questions largely depend on the type of tweets collected. After analyzing classifier accuracy, adjustments to tweet collection methods can be made to potentially increase accuracy/lower overfitting, e.g. lang='en' parameter may help improve model accuracy but restricts unclassified tweets, we will update to the full-archive Twitter api (only 250 requests/mo allowed) once we verify functionality of classifier, we can filter out tweets with more than a set threshold of tickers ($) to get more accurate sentiments on a specific public company, and we can use emoji supported sentiment lexicon for initial polarity score label for our sentiment classifier."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "joLwCQbNC6qX"
      },
      "source": [
        "<a id=\"2\"></a>\n",
        "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u6podAJ4C6qX"
      },
      "source": [
        "## 2. METHOD"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "frpx1WYYC6qX"
      },
      "source": [
        "### 2.1. Data Acquisition\n",
        "\n",
        "There are two datasets used, a financial dataset which includes 5 years of stock data and derived financial metrics for various companies (~1500 rows), as well as a dataset of tweets tweets (~1000 tweets for now). The tweets dataset variables are date, tweet, and sentiment score. \n",
        "\n",
        "For the financial dataset, the data is quantitative and recorded for each trading day. The variables are as follows: \n",
        "- Date - trading day date in YYYY-MM-DD format\n",
        "- Close - stock price at the end of the trading day\n",
        "- Open - stock price at the beginning of the trading day\n",
        "- High - stock's highest price during the trading day\n",
        "- Low - stock's lowest price during the trading day\n",
        "- Volume - amount of stock traded\n",
        "- Change % - percent change in price from open to close\n",
        "- Dividends - dividends paid on trading day\n",
        "- Stock Splits - ratio of shares obtained to previous during stock split event\n",
        "- HL % - difference between high and low price\n",
        "- HPR - return from previous day including profit (dividends)\n",
        "- Market Capitalization - total market value of equity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5LtskNp0C6qX"
      },
      "source": [
        "### 2.2. Data Analysis\n",
        "\n",
        "The target for the tweets dataset is a polarity score, Sentiment Score, provided by an external sentiment lexicon and corpus (AFINN).\n",
        "\n",
        "To analyze our data, we are tackling classification. The sentiment analysis classifier will vectorize tweet text and predict a sentiment class: positive, neutral, or negative.\n",
        "\n",
        "For the sentiment classifier, the following algorithms will be utilized: LinearSVC, MultinomialNB, and LogisticRegression. We chose to focus on these algorithms (other than DecisionTreeRegressor)  because the following source mentions they are the most effective to tackle problems regarding text sentiment: A. Pak and P. Paroubek. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. Lrec, pages 1320–1326, 2010., Random Forest - type of decision tree algorithm, https://link.springer.com/article/10.1007/s10796-021-10135-7.\n",
        "\n",
        "- LinearSVC - Support Vector Classification: works by using a linear kernel to find the best fit hyperplane (maximized margin) which splits data into classes\n",
        "- MultinomialNB - Naive Bayes: Bayesian classification algorithm suited for discrete features\n",
        "- LogisticRegression: uses a logistic model e.g. sigmoid to predict binomial classes\n",
        "\n",
        "For the sentiment classifier we are utilizing tf-idf with n-grams to vectorize the tweets into a matrix of many adjacent word-combination features. N-grams help cover combinations of words that may have different meanings and sentiment together than apart (e.g. “bad not good” vs “good not bad”). We will also remove stopwords to combat overfitting. We are planning on normalizing stock price data using a standard scaler, because stock prices are based on shares outstanding and aren’t reflective of the market capitalization (true market value of equity). \n",
        "\n",
        "It would be useful to visualize stock price data for a stock to get the gist of the price trend for a particular ticker. Additionally, a scatter matrix can be used to visualize correlations between features, which is also useful for identifying which features break conditional independence for Naive Bayes classification algorithms. Also included but not one of the two visualizations is a word cloud using the wordcloud library just to indicate the most common words among our tweet corpus. It also shows that restricting multiple tickers within tweets would be a good idea, as we don’t want overall sentiment to be masked by large numbers of other companies."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gBcxVuP0C6qY"
      },
      "source": [
        "<a id=\"3\"></a>\n",
        "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wXIqCblWC6qY"
      },
      "source": [
        "## 3. RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX3I2BnZC6qY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import re\n",
        "import plotly.express as px\n",
        "import stylecloud\n",
        "from afinn import Afinn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ochsyCbzC6qZ"
      },
      "source": [
        "### 3.1. Data Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYw7c-5nC6qZ",
        "outputId": "885e0d51-6a63-45dc-c435-8f67df9d81b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Original Text</th>\n",
              "      <th>Clean Text</th>\n",
              "      <th>Important Text</th>\n",
              "      <th>Sentiment Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-11-14 09:23:49+00:00</td>\n",
              "      <td>most profitable crypto group\\n\\n$RCAT $NAKD $C...</td>\n",
              "      <td>most profitable crypto group\\n\\n$rcat $nakd $c...</td>\n",
              "      <td>profitable crypto group rcat nakd cei fami bbi...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-11-14 09:17:13+00:00</td>\n",
              "      <td>5 Advanced Secrets Every Options Trader Should...</td>\n",
              "      <td>5 advanced secrets every options trader should...</td>\n",
              "      <td>5 advanced secrets every options trader know s...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-11-14 09:06:40+00:00</td>\n",
              "      <td>✅Stocks \\n✅Options \\n✅Day trading \\n\\n$CLDR $N...</td>\n",
              "      <td>✅stocks \\n✅options \\n✅day trading \\n\\n$cldr $n...</td>\n",
              "      <td>✅stocks ✅options ✅day trading cldr nok abev zn...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-11-14 09:00:51+00:00</td>\n",
              "      <td>$NVDA　圧倒的。\\n\\nFB復活するかな。\\n\\n$AAPL\\n$AMZN\\n$FB\\n...</td>\n",
              "      <td>$nvda　圧倒的。\\n\\nfb復活するかな。\\n\\n$aapl\\n$amzn\\n$fb\\n...</td>\n",
              "      <td>nvda 圧倒的。 fb復活するかな。 aapl amzn fb goog msft nvda</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-11-14 09:00:37+00:00</td>\n",
              "      <td>$ATOM call, its a key level, trade safe..... \\...</td>\n",
              "      <td>$atom call, its a key level, trade safe..... \\...</td>\n",
              "      <td>atom call , key level , trade safe ..... soul ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "TWEETS_URL = 'https://raw.githubusercontent.com/sisolta/ds3000/main/initial_tweets.csv'\n",
        "tweets = pd.read_csv(TWEETS_URL, index_col=0)\n",
        "tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB5AkB6aC6qa",
        "outputId": "a389ceed-2a35-4749-c3d8-b7a546b1a597"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              GOOG                                                       \\\n",
            "              Open    High     Low   Close Adj Close   Volume Dividends   \n",
            "Date                                                                      \n",
            "2016-12-09  780.00  789.43  779.02  789.29    789.29  1821900         0   \n",
            "2016-12-12  785.04  791.25  784.35  789.27    789.27  2104100         0   \n",
            "2016-12-13  793.90  804.38  793.34  796.10    796.10  2145200         0   \n",
            "2016-12-14  797.40  804.00  794.01  797.07    797.07  1704200         0   \n",
            "2016-12-15  797.34  803.00  792.92  797.85    797.85  1626500         0   \n",
            "\n",
            "                           AMZN          ...      TSLA                AAPL  \\\n",
            "           Stock Splits    Open    High  ... Dividends Stock Splits   Open   \n",
            "Date                                     ...                                 \n",
            "2016-12-09            0  770.00  770.25  ...         0          0.0  28.08   \n",
            "2016-12-12            0  766.40  766.89  ...         0          0.0  28.32   \n",
            "2016-12-13            0  764.96  782.46  ...         0          0.0  28.46   \n",
            "2016-12-14            0  778.25  780.86  ...         0          0.0  28.76   \n",
            "2016-12-15            0  766.28  769.10  ...         0          0.0  28.84   \n",
            "\n",
            "                                                                             \n",
            "             High    Low  Close Adj Close     Volume Dividends Stock Splits  \n",
            "Date                                                                         \n",
            "2016-12-09  28.67  28.08  28.49     26.81  137610400       0.0          0.0  \n",
            "2016-12-12  28.75  28.12  28.33     26.66  105497600       0.0          0.0  \n",
            "2016-12-13  28.98  28.44  28.80     27.11  174935200       0.0          0.0  \n",
            "2016-12-14  29.05  28.75  28.80     27.11  136127200       0.0          0.0  \n",
            "2016-12-15  29.18  28.81  28.95     27.25  186098000       0.0          0.0  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>Stock Splits</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-12-09</th>\n",
              "      <td>28.08</td>\n",
              "      <td>28.67</td>\n",
              "      <td>28.08</td>\n",
              "      <td>28.49</td>\n",
              "      <td>26.81</td>\n",
              "      <td>137610400</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-12</th>\n",
              "      <td>28.32</td>\n",
              "      <td>28.75</td>\n",
              "      <td>28.12</td>\n",
              "      <td>28.33</td>\n",
              "      <td>26.66</td>\n",
              "      <td>105497600</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-13</th>\n",
              "      <td>28.46</td>\n",
              "      <td>28.98</td>\n",
              "      <td>28.44</td>\n",
              "      <td>28.80</td>\n",
              "      <td>27.11</td>\n",
              "      <td>174935200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-14</th>\n",
              "      <td>28.76</td>\n",
              "      <td>29.05</td>\n",
              "      <td>28.75</td>\n",
              "      <td>28.80</td>\n",
              "      <td>27.11</td>\n",
              "      <td>136127200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-15</th>\n",
              "      <td>28.84</td>\n",
              "      <td>29.18</td>\n",
              "      <td>28.81</td>\n",
              "      <td>28.95</td>\n",
              "      <td>27.25</td>\n",
              "      <td>186098000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "STOCKS_URL = 'https://raw.githubusercontent.com/sisolta/ds3000/main/stocks_historical.csv'\n",
        "stocks_historical = pd.read_csv('stocks_historical.csv', index_col=0, header=[0,1])\n",
        "print(stocks_historical.head())\n",
        "aapl = stocks_historical['AAPL']\n",
        "aapl.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l0TodRdPC6qa"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XmX6SZ5-C6qa"
      },
      "source": [
        "We are preprocessing tweets to clear potential disruptors to our sentiment analysis model. We are looking to clear mentions, links, eliminate punctuation, and remove the hashtag symbol. We are not removing the actual hashtags as they usually contain important identifiers relating to the overall tweet sentiment (this was done in our dataset generation code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIUvM6UFC6qa",
        "outputId": "8a965849-5166-4540-cb06-d3f5f0fccb1f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Original Text</th>\n",
              "      <th>Clean Text</th>\n",
              "      <th>Important Text</th>\n",
              "      <th>Sentiment Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-11-14 09:23:49+00:00</td>\n",
              "      <td>most profitable crypto group\\n\\n$RCAT $NAKD $C...</td>\n",
              "      <td>most profitable crypto group\\n\\n$rcat $nakd $c...</td>\n",
              "      <td>profitable crypto group rcat nakd cei fami bbi...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-11-14 09:17:13+00:00</td>\n",
              "      <td>5 Advanced Secrets Every Options Trader Should...</td>\n",
              "      <td>5 advanced secrets every options trader should...</td>\n",
              "      <td>5 advanced secrets every options trader know s...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-11-14 09:06:40+00:00</td>\n",
              "      <td>✅Stocks \\n✅Options \\n✅Day trading \\n\\n$CLDR $N...</td>\n",
              "      <td>✅stocks \\n✅options \\n✅day trading \\n\\n$cldr $n...</td>\n",
              "      <td>✅stocks ✅options ✅day trading cldr nok abev zn...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-11-14 09:00:51+00:00</td>\n",
              "      <td>$NVDA　圧倒的。\\n\\nFB復活するかな。\\n\\n$AAPL\\n$AMZN\\n$FB\\n...</td>\n",
              "      <td>$nvda　圧倒的。\\n\\nfb復活するかな。\\n\\n$aapl\\n$amzn\\n$fb\\n...</td>\n",
              "      <td>nvda 圧倒的。 fb復活するかな。 aapl amzn fb goog msft nvda</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-11-14 09:00:37+00:00</td>\n",
              "      <td>$ATOM call, its a key level, trade safe..... \\...</td>\n",
              "      <td>$atom call, its a key level, trade safe..... \\...</td>\n",
              "      <td>atom call , key level , trade safe ..... soul ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# regex for pattern matching url\n",
        "# source: https://github.com/Traumatizn/RegEx/blob/main/Python/Url_Pattern.md\n",
        "url_pattern = r'((?:(?<=[^a-zA-Z0-9]){0,}(?:(?:https?\\:\\/\\/){0,1}(?:[a-zA-Z0-9\\%]{1,}\\:[a-zA-Z0-9\\%]{1,}[@]){,1})(?:(?:\\w{1,}\\.{1}){1,5}(?:(?:[a-zA-Z]){1,})|(?:[a-zA-Z]{1,}\\/[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\:[0-9]{1,4}){1})){1}(?:(?:(?:\\/{0,1}(?:[a-zA-Z0-9\\-\\_\\=\\-]){1,})*)(?:[?][a-zA-Z0-9\\=\\%\\&\\_\\-]{1,}){0,1})(?:\\.(?:[a-zA-Z0-9]){0,}){0,1})'\n",
        "\n",
        "# preprocesses tweets\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet_all_lower = tweet.lower()\n",
        "    tweet_no_mentions = re.sub(r'@[A-Za-z0-9_]+', '', tweet_all_lower)\n",
        "    tweet_no_links = re.sub(url_pattern, '', tweet_no_mentions)\n",
        "    tweet_no_hashtag_symbol = re.sub(r'#', '', tweet_no_links)\n",
        "    return tweet_no_hashtag_symbol\n",
        "\n",
        "tweets.set_index('Date')\n",
        "tweets['Clean Text'] = tweets['Original Text'].apply(preprocess_tweet)\n",
        "tweets.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai7399lZC6qa"
      },
      "source": [
        "We are preprocessing stock data as stock price isn't indicative of firm value, and normalizing these values will help generalize the model to other stock tickers. Standard scaler normalizes each column by symetrically centering the data around the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89rDIHvSC6qb",
        "outputId": "81e1b850-fca4-485f-83b5-98926c142e2f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>Stock Splits</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-12-09</th>\n",
              "      <td>-1.138828</td>\n",
              "      <td>-1.130382</td>\n",
              "      <td>-1.133433</td>\n",
              "      <td>-1.129279</td>\n",
              "      <td>-1.127806</td>\n",
              "      <td>0.294495</td>\n",
              "      <td>-0.126194</td>\n",
              "      <td>-0.028194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-12</th>\n",
              "      <td>-1.132731</td>\n",
              "      <td>-1.128374</td>\n",
              "      <td>-1.132405</td>\n",
              "      <td>-1.133341</td>\n",
              "      <td>-1.131568</td>\n",
              "      <td>-0.277693</td>\n",
              "      <td>-0.126194</td>\n",
              "      <td>-0.028194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-13</th>\n",
              "      <td>-1.129175</td>\n",
              "      <td>-1.122601</td>\n",
              "      <td>-1.124177</td>\n",
              "      <td>-1.121409</td>\n",
              "      <td>-1.120281</td>\n",
              "      <td>0.959551</td>\n",
              "      <td>-0.126194</td>\n",
              "      <td>-0.028194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-14</th>\n",
              "      <td>-1.121555</td>\n",
              "      <td>-1.120844</td>\n",
              "      <td>-1.116207</td>\n",
              "      <td>-1.121409</td>\n",
              "      <td>-1.120281</td>\n",
              "      <td>0.268067</td>\n",
              "      <td>-0.126194</td>\n",
              "      <td>-0.028194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-15</th>\n",
              "      <td>-1.119523</td>\n",
              "      <td>-1.117582</td>\n",
              "      <td>-1.114664</td>\n",
              "      <td>-1.117602</td>\n",
              "      <td>-1.116770</td>\n",
              "      <td>1.158450</td>\n",
              "      <td>-0.126194</td>\n",
              "      <td>-0.028194</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# normalize stock data\n",
        "def normalize_data(df):\n",
        "    return pd.DataFrame(StandardScaler().fit_transform(df), index=df.index, columns=df.columns)\n",
        "\n",
        "aapl_normalized = normalize_data(aapl)\n",
        "aapl_normalized.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1Rcvh_C6qb"
      },
      "source": [
        "#### Feature Extraction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnVUJ8LnC6qb"
      },
      "source": [
        "We are performing feature extraction on tweets by cleaning the tweet and defining a target for supervised learning. Here, we remove stopwords and ticker signs and use an external lexicon (afinn) to score the tweet's sentiment. This was done in the dataset generation jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9GxLurlC6qb",
        "outputId": "9002b441-6a11-4a2b-b8f1-c46d5a404315"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Original Text</th>\n",
              "      <th>Clean Text</th>\n",
              "      <th>Important Text</th>\n",
              "      <th>Sentiment Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-11-14 09:23:49+00:00</td>\n",
              "      <td>most profitable crypto group\\n\\n$RCAT $NAKD $C...</td>\n",
              "      <td>most profitable crypto group\\n\\n$rcat $nakd $c...</td>\n",
              "      <td>profitable crypto group rcat nakd cei fami bbi...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-11-14 09:17:13+00:00</td>\n",
              "      <td>5 Advanced Secrets Every Options Trader Should...</td>\n",
              "      <td>5 advanced secrets every options trader should...</td>\n",
              "      <td>5 advanced secrets every options trader know s...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-11-14 09:06:40+00:00</td>\n",
              "      <td>✅Stocks \\n✅Options \\n✅Day trading \\n\\n$CLDR $N...</td>\n",
              "      <td>✅stocks \\n✅options \\n✅day trading \\n\\n$cldr $n...</td>\n",
              "      <td>✅stocks ✅options ✅day trading cldr nok abev zn...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-11-14 09:00:51+00:00</td>\n",
              "      <td>$NVDA　圧倒的。\\n\\nFB復活するかな。\\n\\n$AAPL\\n$AMZN\\n$FB\\n...</td>\n",
              "      <td>$nvda　圧倒的。\\n\\nfb復活するかな。\\n\\n$aapl\\n$amzn\\n$fb\\n...</td>\n",
              "      <td>nvda 圧倒的。 fb復活するかな。 aapl amzn fb goog msft nvda</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-11-14 09:00:37+00:00</td>\n",
              "      <td>$ATOM call, its a key level, trade safe..... \\...</td>\n",
              "      <td>$atom call, its a key level, trade safe..... \\...</td>\n",
              "      <td>atom call , key level , trade safe ..... soul ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# clean tweet and score it using external lexicon for target in supervised learning\n",
        "afinn = Afinn()\n",
        "def filter_and_score(clean_tweet):\n",
        "    # tokenize and filter tweet\n",
        "    tweet_tokens = [word for word in word_tokenize(clean_tweet) if word not in stopwords.words('english') and not word == '$']\n",
        "    important_text = ' '.join(tweet_tokens)\n",
        "    # score tweet - sentiment score b/w -6 and +6 based on positivity of tweet\n",
        "    # -1 = negative, 0 = neutral, 1 = positive transformation\n",
        "    score = afinn.score()\n",
        "    return important_text, score\n",
        "\n",
        "# done in dataset generation notebook, nltk modules must be installed (large)\n",
        "# tweets['Important Text'], tweets['Sentiment Score'] = zip(*tweets['Clean Text'].map(filter_and_score))\n",
        "tweets.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0si70YAzC6qb"
      },
      "source": [
        "We are performing feature extraction on stock tweets to create new columns to represent important financial metrics, namely percent change, holding period return, high low percentage, and market capitalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVHF-evZC6qb",
        "outputId": "8dfaae25-7c42-4d92-9ac3-c5e413d0a556"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"10\" halign=\"left\">AAPL</th>\n",
              "      <th>...</th>\n",
              "      <th colspan=\"10\" halign=\"left\">TSLA</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Change %</th>\n",
              "      <th>Close</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>HL %</th>\n",
              "      <th>HPR</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Mkt Cap ($B)</th>\n",
              "      <th>Open</th>\n",
              "      <th>...</th>\n",
              "      <th>Close</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>HL %</th>\n",
              "      <th>HPR</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Mkt Cap ($B)</th>\n",
              "      <th>Open</th>\n",
              "      <th>Stock Splits</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-12-12</th>\n",
              "      <td>26.66</td>\n",
              "      <td>0.04</td>\n",
              "      <td>28.33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.24</td>\n",
              "      <td>-26.30</td>\n",
              "      <td>28.75</td>\n",
              "      <td>28.12</td>\n",
              "      <td>464.79</td>\n",
              "      <td>28.32</td>\n",
              "      <td>...</td>\n",
              "      <td>38.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.67</td>\n",
              "      <td>-95.12</td>\n",
              "      <td>38.88</td>\n",
              "      <td>38.24</td>\n",
              "      <td>38.65</td>\n",
              "      <td>38.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12194500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-13</th>\n",
              "      <td>27.11</td>\n",
              "      <td>1.19</td>\n",
              "      <td>28.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.90</td>\n",
              "      <td>-25.18</td>\n",
              "      <td>28.98</td>\n",
              "      <td>28.44</td>\n",
              "      <td>472.50</td>\n",
              "      <td>28.46</td>\n",
              "      <td>...</td>\n",
              "      <td>39.63</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.30</td>\n",
              "      <td>-95.02</td>\n",
              "      <td>40.26</td>\n",
              "      <td>38.60</td>\n",
              "      <td>39.80</td>\n",
              "      <td>38.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34119500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-14</th>\n",
              "      <td>27.11</td>\n",
              "      <td>0.14</td>\n",
              "      <td>28.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.04</td>\n",
              "      <td>-27.33</td>\n",
              "      <td>29.05</td>\n",
              "      <td>28.75</td>\n",
              "      <td>472.50</td>\n",
              "      <td>28.76</td>\n",
              "      <td>...</td>\n",
              "      <td>39.74</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.18</td>\n",
              "      <td>-95.01</td>\n",
              "      <td>40.60</td>\n",
              "      <td>39.35</td>\n",
              "      <td>39.91</td>\n",
              "      <td>39.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20754500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-15</th>\n",
              "      <td>27.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>28.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.28</td>\n",
              "      <td>-27.15</td>\n",
              "      <td>29.18</td>\n",
              "      <td>28.81</td>\n",
              "      <td>474.97</td>\n",
              "      <td>28.84</td>\n",
              "      <td>...</td>\n",
              "      <td>39.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.70</td>\n",
              "      <td>-95.05</td>\n",
              "      <td>40.15</td>\n",
              "      <td>39.48</td>\n",
              "      <td>39.69</td>\n",
              "      <td>39.68</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16098000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-12-16</th>\n",
              "      <td>27.29</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>28.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.73</td>\n",
              "      <td>-26.64</td>\n",
              "      <td>29.12</td>\n",
              "      <td>28.91</td>\n",
              "      <td>475.62</td>\n",
              "      <td>29.12</td>\n",
              "      <td>...</td>\n",
              "      <td>40.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.53</td>\n",
              "      <td>-94.88</td>\n",
              "      <td>40.52</td>\n",
              "      <td>39.52</td>\n",
              "      <td>40.67</td>\n",
              "      <td>39.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18984500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 48 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# add features to stock data\n",
        "# move ticker info to subindex date\n",
        "stocks_historical = stocks_historical.stack(level=0)\n",
        "\n",
        "# add key metrics as additional features\n",
        "# percent change\n",
        "stocks_historical['Change %'] = (stocks_historical['Close'] - stocks_historical['Open']) / stocks_historical['Open'] * 100\n",
        "# holding period return\n",
        "stocks_historical['HPR'] = (stocks_historical['Close'] - stocks_historical.shift(1)['Close'] + stocks_historical['Dividends']) / stocks_historical.shift(1)['Close'] * 100\n",
        "# high low percentage\n",
        "stocks_historical['HL %'] = (stocks_historical['High'] - stocks_historical['Low']) / stocks_historical['Low'] * 100\n",
        "\n",
        "# restructure index so tickers appear together as first column level\n",
        "stocks_historical = stocks_historical.unstack().swaplevel(0, 1, axis=1)\n",
        "# drop first row - doesn't have calculated HPR\n",
        "stocks_historical.drop(index=stocks_historical.index[0], axis=0, inplace=True)\n",
        "\n",
        "# brute force market capitalization\n",
        "get_current_shares = lambda ticker: yf.Ticker(ticker).info.get('sharesOutstanding', 0) / 1e9\n",
        "# stocks_historical = stocks_historical.assign(MktCap=get_current_market_cap)\n",
        "for ticker in stocks_historical.columns.unique(level=0).values:\n",
        "    stocks_historical[ticker, 'Mkt Cap ($B)'] = get_current_shares(ticker) * stocks_historical[ticker, 'Close']\n",
        "stocks_historical = stocks_historical.sort_index(axis=1).round(2)\n",
        "stocks_historical.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tzWh7dAeC6qc"
      },
      "source": [
        "#### Data Wrangling: Extract Features & Target"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ty57polHC6qc"
      },
      "source": [
        "We are extracting the features and target for our tweet sentiment classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhxAJgBrC6qc",
        "outputId": "4bfe8be6-50ee-4f8c-c167-dd333e7052d8"
      },
      "outputs": [
        {
          "data": {
            "text/html": []
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# create features and target for text dataframe\n",
        "def features_and_target(df, features, target, target_fx):\n",
        "    features = tweets[features]\n",
        "    target = tweets[target].apply(target_fx)\n",
        "    return features, target\n",
        "\n",
        "features, target = features_and_target(tweets, 'Important Text', 'Sentiment Score', lambda x: 0 if x == 0 else abs(x) / x)\n",
        "features.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SQjtbX28C6qc"
      },
      "source": [
        "### 3.2. Data Exploration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RzuEorCHC6qc"
      },
      "source": [
        "#### Line Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOCVBEX_C6qc"
      },
      "outputs": [],
      "source": [
        "fig = px.line(stocks_historical['AAPL'].reset_index(), x='Date', y='Close', title=\"AAPL Historical Closing Price\")\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W0bZAAfPC6qd"
      },
      "source": [
        "<a href=\"https://ibb.co/6BjKqY6\"><img src=\"https://i.ibb.co/2NGzpqX/Screen-Shot-2021-12-09-at-4-00-47-PM.png\" alt=\"Screen-Shot-2021-12-09-at-4-00-47-PM\" border=\"0\"></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_KnlfhE7C6qd"
      },
      "source": [
        "*Figure I: Line chart of financial dataset which displays the historical close price of Apple Inc. (AAPL) over 5 years.*\n",
        "\n",
        "*Variables: Date, Close*\n",
        "\n",
        "*The line chart shows a gradual increase in price, and much higher growth rates and volatilities when COVID hit.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv_dgjQ4C6qe"
      },
      "source": [
        "#### Scatter Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e084uMXVC6qe"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = px.scatter_matrix(stocks_historical['AAPL'],\n",
        "    dimensions=[\"High\", 'HPR', 'HL %', 'Mkt Cap ($B)'])\n",
        "fig.show()\n",
        "fig.update_layout(\n",
        "    width=1600,\n",
        "    height=1600,\n",
        "    hovermode='closest',\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fl_aDE9GC6qe"
      },
      "source": [
        "<img src=\"https://i.ibb.co/4TH4S7V/image2.png\" alt=\"image2\" border=\"0\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N4Wl1hllC6qe"
      },
      "source": [
        "*Figure II: part of a scatter matrix visualizing correlations between features in the financial dataset.*\n",
        "\n",
        "*Variables: High, HPR, HL %, Mkt Cap ($B)*\n",
        "\n",
        "*Here, we can identify features that break conditional independence for Naive Bayes classification algorithms as their associated scatterplot with a differently-named feature will be linear.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TdTCUYGzC6qe"
      },
      "source": [
        "#### Wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYSjV856C6qe"
      },
      "outputs": [],
      "source": [
        "stylecloud.gen_stylecloud(text=' '.join(list(tweets['Important Text'])), max_words=100, output_name=\"vis_tweets_wc.png\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dUwAeNv3C6qe"
      },
      "source": [
        "<img src=\"https://i.ibb.co/pfdYnwg/vis-tweets-wc.png\" alt=\"vis-tweets-wc\" border=\"0\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxw_8i0mC6qe"
      },
      "source": [
        "*Figure III: wordcloud for current tweet corpus for ‘\\\\$AAPL’ search.*\n",
        "\n",
        "*Variables: Important Text*\n",
        "\n",
        "*The wordcloud shows many other stock tickers. This suggests to exclude tweets with other tickers such as ‘\\\\$AAPL \\\\$TSLA \\\\$SPY’ to increase sentiment classifier accuracy.*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tNzjNdVIC6qe"
      },
      "source": [
        "#### Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe6q6N3KC6qe",
        "outputId": "4e64ce32-83cc-48e8-d56c-ded1e287c5b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Component1  Component2  target\n",
            "0    0.113307    0.148365     1.0\n",
            "1    0.155884    0.115688     1.0\n",
            "2    0.159520    0.093583     1.0\n",
            "3    0.212081    0.150393     0.0\n",
            "4    0.141513    0.102058     1.0\n"
          ]
        },
        {
          "data": {
            "text/html": []
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# reduce into two dimensions\n",
        "# TruncatedSVD uses singular value decomposition (SVD) to reduce dimensions\n",
        "# The algorithm works on sparse matrices as well as fractional data returned by the tfidf vectorizer\n",
        "# Latent semantic analysis (LSA)\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "svd = TruncatedSVD(n_components= 2, random_state=3000)\n",
        "\n",
        "# standardize the features so they are all on the same scale\n",
        "vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(features)\n",
        "features_vect = vect.transform(features)\n",
        "\n",
        "# transform the standardized features using the SVD algorithm \n",
        "reduced_data = svd.fit_transform(features_vect)\n",
        "\n",
        "# rename columns       \n",
        "reduced_df = pd.DataFrame(reduced_data, columns = [\"Component1\", \"Component2\"])\n",
        "reduced_df[\"target\"] = target\n",
        "print(reduced_df[:5])\n",
        "svd.components_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP6adf-OC6qe"
      },
      "outputs": [],
      "source": [
        "graph = px.scatter(reduced_df, x='Component1', y='Component2', color = 'target')\n",
        "graph.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8potBN1zC6qe"
      },
      "source": [
        "<img src=\"https://i.ibb.co/PwjmJpF/newplot.png\" alt=\"newplot\" border=\"0\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "04sK0K4_C6qe"
      },
      "source": [
        "*Dimensionality reduction of tweet text into two components pictures as a scatter plot using latent semantic analysis.*\n",
        "\n",
        "*Variables: all --> SVD --> Component1, Component2, target*\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CRARDBVeC6qe"
      },
      "source": [
        "### 3.3. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEBgBF-7C6qe"
      },
      "outputs": [],
      "source": [
        "# splits features and target into training and testing data\n",
        "def split_train_test(features, target):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=3000)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# create training and testing data\n",
        "X_train, X_test, y_train, y_test = split_train_test(features, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-bWPE8_C6qe"
      },
      "outputs": [],
      "source": [
        "# define classifiers\n",
        "classifiers = {\n",
        "    'Multinomial Naive Bayes': MultinomialNB(),\n",
        "    'Support Vector Machine': LinearSVC(),\n",
        "    'Logistic Regression': LogisticRegression()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jZe6FEHC6qf"
      },
      "outputs": [],
      "source": [
        "# vectorize and create vocabulary\n",
        "# utilize n-grams to preserve context\n",
        "vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
        "\n",
        "#encode the words in X_train and X_test based on the vocabulary\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "X_test_vectorized = vect.transform(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jm3Z6tI8C6qf"
      },
      "source": [
        "#### All Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT4AJA1yC6qf",
        "outputId": "b14b4d9f-bf75-4ab0-d744-cddce0f6ebc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multinomial Naive Bayes\n",
            "\tClassification accuracy on training set: 0.812\n",
            "\tClassification accuracy on testing set: 0.720\n",
            "\tf1-score on training set: 0.798\n",
            "\tf1-score on testing set: 0.699\n",
            "\n",
            "Support Vector Machine\n",
            "\tClassification accuracy on training set: 0.913\n",
            "\tClassification accuracy on testing set: 0.720\n",
            "\tf1-score on training set: 0.911\n",
            "\tf1-score on testing set: 0.710\n",
            "\n",
            "Logistic Regression\n",
            "\tClassification accuracy on training set: 0.864\n",
            "\tClassification accuracy on testing set: 0.732\n",
            "\tf1-score on training set: 0.856\n",
            "\tf1-score on testing set: 0.714\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# train classifiers and print results\n",
        "def classifiers_percentage_split(classifiers, X_train, X_test, y_train, y_test):\n",
        "    for classifier_name, classifier_object in classifiers.items():\n",
        "        # train the classifier\n",
        "        model = classifier_object.fit(X=X_train, y=y_train)\n",
        "        print(classifier_name)\n",
        "        # classification accuracy metric\n",
        "        print(f'\\tClassification accuracy on training set: {model.score(X_train, y_train):.3f}')\n",
        "        print(f'\\tClassification accuracy on testing set: {model.score(X_test, y_test):.3f}')\n",
        "        # f1-score metric\n",
        "        print(f'\\tf1-score on training set: {f1_score(y_true=y_train, y_pred=model.predict(X_train), average=\"weighted\"):.3f}')\n",
        "        print(f'\\tf1-score on testing set: {f1_score(y_true=y_test, y_pred=model.predict(X_test), average=\"weighted\"):.3f}\\n')\n",
        "\n",
        "classifiers_percentage_split(classifiers, X_train_vectorized, X_test_vectorized, y_train, y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk2Fr1R-C6qf"
      },
      "source": [
        "The algorithms performed well on the training set, but performed significantly worse on the testing set. This indicates that the model is overfitted."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0j1O26C6qf"
      },
      "source": [
        "#### Selected Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDvugfaEC6qf",
        "outputId": "f39f1b20-5a4a-488e-a21b-95d8fec8b2d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multinomial Naive Bayes\n",
            "\tClassification accuracy on training set: 0.717\n",
            "\tClassification accuracy on testing set: 0.688\n",
            "\tf1-score on training set: 0.686\n",
            "\tf1-score on testing set: 0.653\n",
            "\n",
            "Support Vector Machine\n",
            "\tClassification accuracy on training set: 0.803\n",
            "\tClassification accuracy on testing set: 0.740\n",
            "\tf1-score on training set: 0.794\n",
            "\tf1-score on testing set: 0.727\n",
            "\n",
            "Logistic Regression\n",
            "\tClassification accuracy on training set: 0.772\n",
            "\tClassification accuracy on testing set: 0.728\n",
            "\tf1-score on training set: 0.754\n",
            "\tf1-score on testing set: 0.707\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SelectKBest to select the most important features using a chi squared test\n",
        "def SelectKBest_feature_selection(n_features, classifiers, X_train, X_test, y_train, y_test):\n",
        "    select = SelectKBest(score_func=chi2, k=n_features)\n",
        "    select.fit(X_train, y_train)\n",
        "    X_train_selected = select.transform(X_train)\n",
        "    X_test_selected = select.transform(X_test)\n",
        "    # reuse previous function to fit and print results\n",
        "    return classifiers_percentage_split(classifiers, X_train_selected, X_test_selected, y_train, y_test)\n",
        "\n",
        "SelectKBest_feature_selection(100, classifiers, X_train_vectorized, X_test_vectorized, y_train, y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-KpKSKibC6qf"
      },
      "source": [
        "### 3.4. Model Optimization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey1siERpC6qf"
      },
      "source": [
        "We are performing model optimization by hyperparameter tuning in order to reduce overfitting and increase generalizability of the classifiers to new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYPcgbzoC6qf"
      },
      "outputs": [],
      "source": [
        "nb_params_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "svm_params_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "log_params_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkrOxuRMC6qf",
        "outputId": "c76aad5b-2c8b-4843-88f7-1e9662b51198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NB best parameters: {'alpha': 0.001}\n",
            "SVM best parameters: {'C': 0.1}\n",
            "Log best parameters: {'C': 1}\n"
          ]
        }
      ],
      "source": [
        "def hyperparameter_tune_algorithms(X_train, y_train):\n",
        "    nb = GridSearchCV(MultinomialNB(), nb_params_grid, cv=5)\n",
        "    nb.fit(X_train, y_train)\n",
        "    print(f'NB best parameters: {nb.best_params_}')\n",
        "    svm = GridSearchCV(LinearSVC(max_iter=10000), svm_params_grid, cv=5)\n",
        "    svm.fit(X_train, y_train)\n",
        "    print(f'SVM best parameters: {svm.best_params_}')\n",
        "    log = GridSearchCV(LogisticRegression(max_iter=10000), log_params_grid, cv=5)\n",
        "    log.fit(X_train, y_train)\n",
        "    print(f'Log best parameters: {log.best_params_}')\n",
        "    return nb, svm, log\n",
        "\n",
        "nb_tuned, svm_tuned, log_tuned = hyperparameter_tune_algorithms(X_train_vectorized, y_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_z_jB58OC6qg"
      },
      "source": [
        "### 3.5. Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ_WWmuOC6qg"
      },
      "outputs": [],
      "source": [
        "# define classifiers with tuned algorithms\n",
        "tuned_classifiers = {\n",
        "    'Multinomial Naive Bayes': nb_tuned,\n",
        "    'Support Vector Machine': svm_tuned,\n",
        "    'Logistic Regression': log_tuned\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DHVbAdO_C6qg"
      },
      "source": [
        "#### All Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdczp2LeC6qg",
        "outputId": "91cb8f3d-869c-47fd-9853-cda2311c9799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multinomial Naive Bayes\n",
            "\tClassification accuracy on testing set: 0.692\n",
            "\tf1-score on testing set: 0.680\n",
            "\n",
            "Support Vector Machine\n",
            "\tClassification accuracy on testing set: 0.728\n",
            "\tf1-score on testing set: 0.708\n",
            "\n",
            "Logistic Regression\n",
            "\tClassification accuracy on testing set: 0.732\n",
            "\tf1-score on testing set: 0.714\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# train classifiers and print results\n",
        "def tuned_classifiers_predict(classifiers, X_train, X_test, y_train, y_test):\n",
        "    for classifier_name, classifier_object in classifiers.items():\n",
        "        # train the classifier\n",
        "        model = classifier_object.fit(X=X_train, y=y_train)\n",
        "        print(classifier_name)\n",
        "        # classification accuracy metric\n",
        "        print(f'\\tClassification accuracy on testing set: {model.score(X_test, y_test):.3f}')\n",
        "        # f1-score metric\n",
        "        print(f'\\tf1-score on testing set: {f1_score(y_true=y_test, y_pred=model.predict(X_test), average=\"weighted\"):.3f}\\n')\n",
        "\n",
        "tuned_classifiers_predict(tuned_classifiers, X_train_vectorized, X_test_vectorized, y_train, y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rpN275GVC6qg"
      },
      "source": [
        "#### Selected Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2YCPjVrC6qg",
        "outputId": "2800c70c-dd77-45ff-956b-32cc47ebdee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multinomial Naive Bayes\n",
            "\tClassification accuracy on testing set: 0.720\n",
            "\tf1-score on testing set: 0.695\n",
            "\n",
            "Support Vector Machine\n",
            "\tClassification accuracy on testing set: 0.740\n",
            "\tf1-score on testing set: 0.730\n",
            "\n",
            "Logistic Regression\n",
            "\tClassification accuracy on testing set: 0.736\n",
            "\tf1-score on testing set: 0.727\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# train classifiers and print results for subset of features\n",
        "def tuned_classifiers_predict_subset(n_features, classifiers, X_train, X_test, y_train, y_test):\n",
        "    select = SelectKBest(score_func=chi2, k=n_features)\n",
        "    select.fit(X_train, y_train)\n",
        "    X_train_selected = select.transform(X_train)\n",
        "    X_test_selected = select.transform(X_test)\n",
        "    # reuse previous function to fit and print results\n",
        "    return tuned_classifiers_predict(classifiers, X_train_selected, X_test_selected, y_train, y_test)\n",
        "\n",
        "tuned_classifiers_predict_subset(100, tuned_classifiers, X_train_vectorized, X_test_vectorized, y_train, y_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "txSZjFFwC6qg"
      },
      "source": [
        "<a id=\"4\"></a>\n",
        "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xDiBufGAC6qg"
      },
      "source": [
        "\n",
        "## 4. DISCUSSION\n",
        "\n",
        "For this project we compared Linear SVC, MultinomialNB, and Logistic Regression. \n",
        "\n",
        "When training with all features, each model performed as follows:\n",
        "  * `Linear SVM`\n",
        "    * `Accuracy (test):  0.720 `\n",
        "    * `F1 (train): 0.911 `\n",
        "    * `F1 (test): 0.710`\n",
        "  * `MultinomialNB`\n",
        "    * `Accuracy (test):  0.720 `\n",
        "    * `F1 (train): 0.798 `\n",
        "    * `F1 (test): 0.699`\n",
        "  * `Logistic Regression`\n",
        "    * `Accuracy (test):  0.732 `\n",
        "    * `F1 (train): 0.856 `\n",
        "    * `F1 (test): 0.714`\n",
        "  \n",
        "All three models showed similar test set accuracy and f1 scores, with Linear SVM and Logistic Regression slightly outperforming Multinomial Naive Bayes. However, while Linear SVM had a relatively high F1 score, this model suffered from the greatest overfitting as demonstrated by the large difference between its train and test set F1 scores.\n",
        "\n",
        "Each model was then trained on the top 100 features selected from the vectorized dataset using a chi-squared test. When trained on this reduced feature set, each model performed similarly but Linear SVM overfitting was significantly reduced. \n",
        "\n",
        "We used grid search cross validation to find the optimal hyperparameters for each model. The optimal alpha value for Multinomial Naive Bayes was 0.001, while the optimal C values for SVM and Logistic Regression were found to be 0.1 and 1, respectively.\n",
        "\n",
        "Training these tuned models on the entire test set yielded results not significantly different from their defaults. \n",
        "\n",
        "Because the best performance was yielded when training a Linear SVM model on 100 selected best features, Linear SVM would be the best choice on a refined dataset. However, Logistic Regression offered better accuracy and F1 scores on unselected datasets. For this reason, both Linear SVM and Logistic Regression would be suitable choices for our predictive model, depending on whether features were selected.\n",
        "\n",
        "Based on the performance of these models, we can conclude that our models are able to predict the sentiment of a tweet to some respect but not to a satisfactory accuracy of at least 90%.\n",
        "\n",
        "To answer our original questions:\n",
        "\n",
        "* What ML model would we use to minimize the classifier accuracy difference between testing and training data to increase generalization of the model?\n",
        "\n",
        "LinearSVM provided the best performance on a dataset with selected features, though it exhibited greater overfitting on the entire dataset. For an unselected dataset, Logistic Regression should be used. Therefore, both of these models are well suited for our application.\n",
        "\n",
        "* Are there any financial data that correlate with each other (e.g. High might correlate with HP%)?\n",
        "\n",
        "The financial data that we analyzed largely did not correlate with each other, with the exception of Market Cap and High price. However, this is not a significant correlation as a high price would be trivially expected to correlate with a larger market cap when looking at a single ticker.\n",
        "\n",
        "* How can we improve the performance of sentiment classification specifically for tweets?\n",
        "\n",
        "Selecting for the 100 best features significantly reduced overfitting for LinearSVM and improved model accuracy for all models tested. Because we only tried the best 100 features, selecting for different numbers of features might further increase performance.\n",
        "  \n",
        "\n",
        "The results of our project indicate that stock tweets can be analyzed with some degree of accuracy. Our project can extract needed information from tweets and achieve accuracies of ~73%. However, the usage of these tweets to predict future stock performance may be dubious. Many of the tweets that we analyzed were clearly posted by bots and many of them tagged several popular symbols in a clear attempt to attract user attention. Because of the large volume of these low-quality tweets, Twitter data may not be entirely reliable. Better cleaning techniques might be employed to only search for tweets posted by humans.\n",
        "\n",
        "We identify little to no ethical issues with this project. The tweets we analyze are public and meant to be analyzed in this manner. However, the stock market is volatile and any results that tweet analysis may produce should not be taken as gospel. Improperly relying on machine learning techniques would be especially harmful to individual retail traders, possibly increasing inequitable outcomes in financial markets.\n",
        "\n",
        "In this project, we used natural language processing techniques to classify the sentiment of stock tweets. Our models presented relatively good performance, but still need to be improved. Future experiments may include, as mentioned, better cleaning techniques to select for genuine human tweets. Tweets can be filtered by the quality of their poster, including location, frequency of posting, and amount of activity on their posts. With better data, we can improve the quality of our predictions. With higher quality predictions, NLP analysis of stock tweets might be applicable in projecting the performance of stocks."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2xv-cdpVC6qg"
      },
      "source": [
        "<a id=\"5\"></a>\n",
        "<hr style=\"height:2px; border:none; color:black; background-color:black;\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "egnCDjUbC6qg"
      },
      "source": [
        "### CONTRIBUTIONS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJrmKdLC6qg"
      },
      "source": [
        "- Sections 1 and 2 (Introduction and Method) were done together.\n",
        "- Section 3 visualizations were split among all of us, Sina did wrangling and model optimization.\n",
        "- Section 4 was done by Byron and Nadia."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "l0TodRdPC6qa",
        "0f1Rcvh_C6qb",
        "tzWh7dAeC6qc",
        "RzuEorCHC6qc",
        "Fv_dgjQ4C6qe",
        "TdTCUYGzC6qe",
        "tNzjNdVIC6qe",
        "jm3Z6tI8C6qf",
        "Re0j1O26C6qf"
      ],
      "name": "DS3000_FP4_Group1.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
