{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ALPE1vw26tUV"
      },
      "source": [
        "<center><h1> Fake News! </h1></center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wRXfEB256tUV"
      },
      "source": [
        "In this assignment, you are going to prototype a fake news detector application. The attached dataset contains headlines from online resources along with a Label indicating whether the headline represents Fake News (0) or Real News (1). Your task is to train an ML model to detect Fake News based on the text included in the headline."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dTa6Ee8n6tUV"
      },
      "source": [
        "Go ahead and import the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61h1BTT06tUV",
        "outputId": "75ebef16-3dc8-4471-9737-3d820bde7dfb",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Chicago Bears have had more starting quart...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When Mitt Romney was governor of Massachusetts...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>McCain opposed a requirement that the governme...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4549</th>\n",
              "      <td>Says Barack Obama promised to halve the defici...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4550</th>\n",
              "      <td>I am the only senator who turned down the stat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4551</th>\n",
              "      <td>There is no system to vet refugees from the Mi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4552</th>\n",
              "      <td>I think its seven or eight of the California s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4553</th>\n",
              "      <td>Says the governor is going around the state ta...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4554 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Headline  Label\n",
              "0     Says the Annies List political group supports ...      0\n",
              "1     Health care reform legislation is likely to ma...      0\n",
              "2     The Chicago Bears have had more starting quart...      1\n",
              "3     When Mitt Romney was governor of Massachusetts...      0\n",
              "4     McCain opposed a requirement that the governme...      1\n",
              "...                                                 ...    ...\n",
              "4549  Says Barack Obama promised to halve the defici...      1\n",
              "4550  I am the only senator who turned down the stat...      1\n",
              "4551  There is no system to vet refugees from the Mi...      0\n",
              "4552  I think its seven or eight of the California s...      0\n",
              "4553  Says the governor is going around the state ta...      0\n",
              "\n",
              "[4554 rows x 2 columns]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/yildirimcaglar/yildirimcaglar.github.io/master/ds3000/fake_news_data.csv\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tY6CAUC6tUW",
        "outputId": "f27e7f88-0510-4489-f5af-f796307e15c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    2501\n",
              "1    2053\n",
              "Name: Label, dtype: int64"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# here are the target value counts\n",
        "data[\"Label\"].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_1bsKx3m6tUW"
      },
      "source": [
        "### Question 1 . \n",
        "\n",
        "As you can see the dataset is not perfectly balanced. For a binary classification problem like this, it's better to have a roughly balanced dataset. Therefore, we will need to downsample the false headlines and use 2050 headlines from each class.\n",
        "\n",
        "Write a function to randomly sample an equal number of true and false headlines from the data dataframe. Your function will be generic and should work with any dataframe as described and illustrated below:\n",
        "\n",
        "- The function should receive the dataframe, name of the grouping column, and the number of samples to be drawn\n",
        "- The function should return a dataframe containing an equal number (n) of each unique value contained in the grouping column (column) randomly selected from the original dataframe (df). \n",
        "- Refer to the sample function call.\n",
        "\n",
        "Hint: You'll need to use the sample method of the dataframe object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A7e0Ukp6tUW"
      },
      "outputs": [],
      "source": [
        "def sample_df_equally_by_group(df, column, n):\n",
        "    sample_dfs = []\n",
        "    col_vals = df[column].unique()\n",
        "    for col in col_vals:\n",
        "        sample_dfs.append(df[df[column] == col].sample(n=n))\n",
        "    return pd.concat(sample_dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7Mv7ozQ6tUW"
      },
      "outputs": [],
      "source": [
        "final_data = sample_df_equally_by_group(df=data, column=\"Label\", n=2050)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du2g3CNq6tUX",
        "outputId": "4a80270e-4a3c-490b-84b1-3d24c6d861ef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2778</th>\n",
              "      <td>We can fix our roads without raising taxes.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2861</th>\n",
              "      <td>The media widely overlooked comments made by f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4191</th>\n",
              "      <td>Says If you compare the Portland Metro area to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>Obama \"shunned the opportunity to talk to sold...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560</th>\n",
              "      <td>Most tips left at Dunkin Donuts dont go to emp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>While introducing Donald Trump, former New Yor...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>You've heard endlessly about waterboarding. It...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3567</th>\n",
              "      <td>Three out of the 18 benchmarks of the (GAO) ha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>The Walton family of Walmart ... This one fami...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>Says right now, we have more military spending...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4100 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Headline  Label\n",
              "2778        We can fix our roads without raising taxes.      0\n",
              "2861  The media widely overlooked comments made by f...      0\n",
              "4191  Says If you compare the Portland Metro area to...      0\n",
              "1922  Obama \"shunned the opportunity to talk to sold...      0\n",
              "560   Most tips left at Dunkin Donuts dont go to emp...      0\n",
              "...                                                 ...    ...\n",
              "2931  While introducing Donald Trump, former New Yor...      1\n",
              "534   You've heard endlessly about waterboarding. It...      1\n",
              "3567  Three out of the 18 benchmarks of the (GAO) ha...      1\n",
              "2765  The Walton family of Walmart ... This one fami...      1\n",
              "551   Says right now, we have more military spending...      1\n",
              "\n",
              "[4100 rows x 2 columns]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bCiznHcv6tUX"
      },
      "source": [
        "Here are the final counts in the sampled dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bepS1ORg6tUX",
        "outputId": "b50ff129-c624-49aa-dd30-419f4eeb043c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    2050\n",
              "1    2050\n",
              "Name: Label, dtype: int64"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_data[\"Label\"].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QDOUMiw36tUX"
      },
      "source": [
        "### Question 2 .\n",
        "\n",
        "Before analyzing the data, you will produce a word cloud for the true and false headlines. A word cloud is a nice way to visualize the frequent words appearing in a piece of text. \n",
        "\n",
        "For this visualization, you're going to use the StyleCloud library:\n",
        " - https://github.com/minimaxir/stylecloud\n",
        "\n",
        "You'll first need to install the library by referring to the documentation.\n",
        "\n",
        "Study the documentation carefully. The first sample shows you how to produce a word cloud from a text file:\n",
        " - https://github.com/minimaxir/stylecloud#usage\n",
        " \n",
        "Instead of specifying a file, you can also specify the text directly. For this purpose, you'll need to use the **text** keyword argument and specify the text that you want to visualize. \n",
        "\n",
        "By default, the word cloud is saved in the save directory as your Notebook file. Once you've executed your code, check that folder. The default file name is \"stylecloud.png\". You can specify the output name using the output_name keyword argument.\n",
        "\n",
        "\n",
        "For this question, produce one word cloud for all true headlines (named \"vis_true_headlines.png\") and another for all false headlines (named \"vis_false_headlines.png\") contained in the final_data dataframe. The names of the files must be specified in your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw6Mzt836tUX"
      },
      "outputs": [],
      "source": [
        "import stylecloud\n",
        "\n",
        "stylecloud.gen_stylecloud(text=' '.join(list(final_data[final_data['Label'] == 1]['Headline'])), max_words=100, output_name=\"vis_true_headlines.png\")\n",
        "\n",
        "stylecloud.gen_stylecloud(text=' '.join(list(final_data[final_data['Label'] == 0]['Headline'])), max_words=100, output_name=\"vis_false_headlines.png\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZRGYWbr6tUX"
      },
      "source": [
        "True headlines:\n",
        "<img src=\"https://i.ibb.co/mXT08Lh/vis-true-headlines.png\" alt=\"vis_true_headlines\" border=\"0\">\n",
        "\n",
        "False headlines:\n",
        "<img src=\"https://i.ibb.co/7RnrLcb/vis-false-headlines.png\" alt=\"vis_false_headlines\" border=\"0\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rtntnqSQ6tUY"
      },
      "source": [
        "### Question 3 .\n",
        "Write a function to get the features and target variables from the final_data dataframe and obtain your training and test splits. The function should receive the dataframe and the names of the feature and target columns. Then it should return the splits as shown in the sample output. Use random_state=3000 when splitting your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPDuTx706tUY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4VKXHgl6tUY"
      },
      "outputs": [],
      "source": [
        "def split_data(df, feature_column, target_column):\n",
        "    return train_test_split(df[feature_column], df[target_column], random_state=3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgwZ6kD76tUY"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = split_data(df=final_data, \n",
        "                                              feature_column=\"Headline\", \n",
        "                                              target_column=\"Label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8m1ZZwi6tUZ",
        "outputId": "c53281fd-3f5d-4d99-ba43-7411f253bc8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3075,)"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-BrYvTk6tUZ",
        "outputId": "501901b9-0917-407c-ab95-3c150ff4d941"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1025,)"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FcKI9-ov6tUZ"
      },
      "source": [
        "### Question 4 .\n",
        "\n",
        "Write a function that can be used to vectorize text using the bag-of-words approach. \n",
        "\n",
        "- The function should receive the training set and testing set features as arguments.\n",
        "- The third argument should be the vectorizer, with two possible values: 'count' for CountVectorizer and 'tfidf' for TfidfVectorizer. The default vectorizer should be tfidf.\n",
        "- The function should construct the vocabulary based on the training set, which should then be used to represent both the training and testing sets. The vectorized training and testing sets should be returned as a tuple at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i61e7-nz6tUZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eExTTuj06tUZ"
      },
      "outputs": [],
      "source": [
        "def text_vectorizer(train_set, test_set, vectorizer):\n",
        "    if vectorizer == 'count':\n",
        "        vect = CountVectorizer().fit(train_set)\n",
        "    else:\n",
        "        vect = TfidfVectorizer().fit(X_train)\n",
        "    train_vect = vect.transform(train_set)\n",
        "    test_vect = vect.transform(test_set)\n",
        "    return (train_vect, test_vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM5H-b276tUZ"
      },
      "outputs": [],
      "source": [
        "X_train_vectorized, X_test_vectorized = text_vectorizer(train_set=X_train, \n",
        "                                                        test_set=X_test, \n",
        "                                                        vectorizer = \"tfidf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH_xc5F16tUZ",
        "outputId": "c3e19fa2-05eb-47bb-ac5b-3142a4083b71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3075, 6867)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_vectorized.toarray().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcj0UZza6tUZ",
        "outputId": "4bf9d3ca-e584-4f6c-ac01-9e897329722b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1025, 6867)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test_vectorized.toarray().shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8l4PfJEC6tUZ"
      },
      "source": [
        "### Question 5 .\n",
        "\n",
        "Write a code snippet to apply LogisticRegression, MultinomialNB, and DecisionTreeClassifier algorithms to the vectorized data. The model performance must be evaluated on the testing set. Your code must use an iteration statement to apply and evaluate multiple algorithms. Refer to the sample output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4yBp7pp6tUZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XJJGMwy6tUa",
        "outputId": "9bdb37b6-ea44-4492-dff5-2d9ce5cda36b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic regression\n",
            "\tClassification accuracy on training set:  0.8653658536585366\n",
            "\tClassification accuracy on testing set:  0.624390243902439\n",
            "\n",
            "\n",
            "Multinomial naive bayes\n",
            "\tClassification accuracy on training set:  0.8878048780487805\n",
            "\tClassification accuracy on testing set:  0.6039024390243902\n",
            "\n",
            "\n",
            "Decision tree\n",
            "\tClassification accuracy on training set:  1.0\n",
            "\tClassification accuracy on testing set:  0.551219512195122\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifiers = {'Logistic regression': LogisticRegression(), \n",
        "               'Multinomial naive bayes': MultinomialNB(), \n",
        "               'Decision tree': DecisionTreeClassifier()}\n",
        "\n",
        "for classifier_name, classifier in classifiers.items():\n",
        "    model = classifier.fit(X=X_train_vectorized, y=y_train)\n",
        "    print(classifier_name)\n",
        "    print(\"\\tClassification accuracy on training set: \", model.score(X_train_vectorized, y_train))\n",
        "    print(\"\\tClassification accuracy on testing set: \", model.score(X_test_vectorized, y_test))\n",
        "    print('\\n')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YZVZS4Ca6tUa"
      },
      "source": [
        "### Question 6 .\n",
        "\n",
        "Based on the quick results from the previous question, it seems that the Logistic Regression is probably the best choice for this problem. For this question, you will train a Logistic Regression algorithm again, but this time you'll modify some of the parameters when extracting the features. More specifically, you should include a word in your vocabulary if it has a minimum document frequency of 5. You should also extract ngrams up to bigrams (which includes both unigrams and bigrams). Finally, you should eliminate English stop words from your vocabulary.\n",
        "\n",
        "Refer to the sample output showing model performance after modifying these parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOR7wi3I6tUa",
        "outputId": "5095f6d8-cdb9-44fd-841a-09d7764be77d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification accuracy on training set:  0.7947967479674797\n",
            "Classification accuracy on testing set:  0.5863414634146341\n"
          ]
        }
      ],
      "source": [
        "#create the vocabulary based on the training data\n",
        "vect = TfidfVectorizer(min_df=5, ngram_range=(1,2), stop_words='english').fit(X_train)\n",
        "\n",
        "#encode the words in X_train and X_test based on the vocabulary\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "X_test_vectorized = vect.transform(X_test)\n",
        "\n",
        "#train the classifier\n",
        "model = LogisticRegression().fit(X=X_train_vectorized, y=y_train)\n",
        "\n",
        "print(\"Classification accuracy on training set: \", model.score(X_train_vectorized, y_train))\n",
        "print(\"Classification accuracy on testing set: \", model.score(X_test_vectorized, y_test))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ibV0JNGJ6tUa"
      },
      "source": [
        "### Question 7 .\n",
        "\n",
        "In this last question, you will write a function that can take a headline as a string and return whether it's Real or Fake News. The function should also return the probability associated with the decision, as a measure of the confidence in the prediction.\n",
        "\n",
        "The prediction must be based on the Logistic Regression model trained in the previous question.\n",
        "\n",
        "Refer to the sample function calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsk1y_fs6tUa"
      },
      "outputs": [],
      "source": [
        "def headline_checker(headline):\n",
        "    headline = vect.transform(headline)\n",
        "    pred = model.predict(headline)\n",
        "    news = 'Real News' if pred == 1 else 'Fake News'\n",
        "    probs = model.predict_proba(headline)\n",
        "    prob = probs[0][1] if pred == 1 else probs[0][0]\n",
        "    print(f'Model classification: {news}')\n",
        "    print(f'Probability: {prob:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4V-zTW66tUb",
        "outputId": "9ad1964b-3ba1-4478-e451-f0ae4eb1b6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model classification: Real News\n",
            "Probability: 0.70\n"
          ]
        }
      ],
      "source": [
        "headline_checker([\"The State adds new vaccine requirement for senate members\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iToCvbPx6tUb",
        "outputId": "69b67579-2c92-462e-aa66-749ebf99f68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model classification: Fake News\n",
            "Probability: 0.83\n"
          ]
        }
      ],
      "source": [
        "headline_checker([\"Wisconsin Governer says he will never campaign again\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DS3000_HW8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
