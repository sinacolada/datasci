{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vhLhPJx1jiUb"
      },
      "source": [
        "<center> <h2> Avengers Tower is Relocating to Boston! </h2> </center>\n",
        "\n",
        "<br />\n",
        "\n",
        "<center><img src = \"https://vignette.wikia.nocookie.net/marvelcinematicuniverse/images/0/09/Avengers_Tower_AoU_cropped.png/revision/latest/scale-to-width-down/289?cb=20170328174622\" width = 200/></center>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOBDjpaDjiUc"
      },
      "source": [
        "You might have been led to believe that **Avengers Tower** has relocated to upstate New York. Don't worry; you're not alone. Tony Stark is smart... ;)\n",
        "\n",
        "It turns out the tower is in fact relocating to Boston (well, it should have been constructed here in the first place anyways. I mean NYC?). :) \n",
        "\n",
        "As his entire automobile fleet is being shipped to Boston, Stark needs your help in learning more about the used car market in Boston, especially given the increasing prices due to the pandemic. It turns out he does need a car and will need to purchase a used car.\n",
        "\n",
        "As usual, Stark made it sound like he wants you to practice your web scraping skills by helping him scrape used car listings from Craigslist (CL). That said, your cynical view is that he is trying to make you do his job! And you're probably right.\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bBNYtfBkjiUd"
      },
      "source": [
        "## Getting started\n",
        "\n",
        "* You will need to scrape the used cars for sale list from https://boston.craigslist.org.\n",
        "* It is probably a very good idea to familiarize yourself with the backend structure of the website before you attempt to scrape data from it! Go ahead and explore the site and inspect the HTML code.\n",
        "\n",
        "\n",
        "**Important advice:**\n",
        "* Focus on each step of web scraping one at a time. If you try to do everything at once, it may get ugly.\n",
        "* Write code for the HW problems you have, not the ones you wish you had."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-s_cOhejiUd",
        "outputId": "1e458d7a-be66-44ee-ba7d-d33ad8b57893"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://boston.craigslist.org'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"https://boston.craigslist.org\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OHDuK_bLjiUf"
      },
      "source": [
        "### Question 1 \n",
        "Although you will be scraping the https://boston.craigslist.org URL, Tony would like you to write a function that takes two arguments, CL homepage link and a category name, and returns the URL of the category page.\n",
        "\n",
        "* The idea is that because CL is available across the country, this function could be used to scrape data from CL for other cities as well.\n",
        "* On CL, cars for sale are posted under \"cars+trucks\" category. If you wanted to scrape data from that page, you would need to access https://boston.craigslist.org/d/cars-trucks/search/cta\n",
        "* Because CL uses the same URL structure across all cities in the country, writing a function might be useful!\n",
        "* Besides, the backend URL conventions on CL do change from time to time, but the category names are rather constant. Thus, using a hard-coded URL could lead to issues down the road (if the page structure should get updated)\n",
        "\n",
        "\n",
        "* Here are some sample function calls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxGBZlnAjiUg"
      },
      "outputs": [],
      "source": [
        "# get_url_from_cl(\"https://boston.craigslist.org\", \"cars+trucks\")\n",
        "# this returns the URL of the \"cars+trucks\" page on boston.craigslist.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CbiCpzajiUg",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# get_url_from_cl(\"https://newyork.craigslist.org\", \"cars+trucks\")\n",
        "# this returns the URL of the \"cars+trucks\" page on newyork.craigslist.com"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5fZ0tSKPjiUg"
      },
      "source": [
        "**Hint:**\n",
        "* your function should take the URL in the first argument and the category name in the second argument.\n",
        "* search for the URL of the second argument on the CL homepage provided in the first argument.\n",
        "* and then return the URL for the category provided in the second argument.\n",
        "* manually concatenating the two arguments based on the above output is not acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV6HVEvLjiUg"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request as urllib\n",
        "from urllib.error import URLError, HTTPError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBXtlEPfjiUh"
      },
      "outputs": [],
      "source": [
        "def get_url_from_cl(cl_url, cl_label):\n",
        "    \"\"\" \n",
        "        gets craigslist url based on location and category\n",
        "            \n",
        "        Args:\n",
        "            cl_url (string): craigslist location url\n",
        "            cl_label (string): category label\n",
        "            \n",
        "        Returns:\n",
        "            string: full craigslist url for location and category\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        html = urllib.urlopen(cl_url)\n",
        "        soup = BeautifulSoup(html.read())\n",
        "        html.close()\n",
        "    except URLError:\n",
        "        return \"invalid url\"\n",
        "    except HTTPError:\n",
        "        return \"request failed\"\n",
        "    \n",
        "    # restrict tags to area with categories\n",
        "    categories = soup.find(\"div\", {\"id\": \"center\"})\n",
        "    \n",
        "    # search area of categories for proper url\n",
        "    # find the a tag with span containing text which matches category label then extract link\n",
        "    try:\n",
        "        cat_url = categories.find(lambda x: x.name == \"a\" and x.span.text.strip() == cl_label).get(\"href\").strip()\n",
        "    except:\n",
        "        return \"category doesn't exist\"\n",
        "    \n",
        "    # concatenate category hyperlink with original for full url\n",
        "    return cl_url + cat_url\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTa2nxPmjiUh",
        "outputId": "1de9fdcb-dd31-4b68-dded-f6ba8ec70812"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://boston.craigslist.org/d/cars-trucks/search/cta'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_url_from_cl(\"https://boston.craigslist.org\", \"cars+trucks\")\n",
        "# this returns the URL of the \"cars+trucks\" page on boston.craigslist.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW3BM6F6jiUh",
        "outputId": "e9448457-7788-4cc7-f5ab-f258150fe49e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://newyork.craigslist.org/d/cars-trucks/search/cta'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_url_from_cl(\"https://newyork.craigslist.org\", \"cars+trucks\")\n",
        "# this returns the URL of the \"cars+trucks\" page on newyork.craigslist.com"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PMxx_SBsjiUh"
      },
      "source": [
        "### Question 2 \n",
        "From the \"cars+trucks\" listings page, you will scrape 4 pieces of information about each post: **listing price of the user card**, **the title of the post**, **when the listing was posted**, and **the link to the post**, as shown below:\n",
        "\n",
        "<center> <img src = \"https://raw.githubusercontent.com/yildirimcaglar/yildirimcaglar.github.io/master/ds3000/CL_Car.png\" width = 300 /> </src>\n",
        "\n",
        "* The good thing is you only need to scrape data from the first page, which by default provides a list of about 120 listings. And this is enough for Stark.\n",
        "\n",
        "* You will store your scraped data in a DataFrame. \n",
        "\n",
        "**Therefore, in this question, you are asked to define an *empty* DataFrame, df_cars, with four columns: Price, Title, Posting Date, and URL.**\n",
        "\n",
        "For now, write a code snippet to simply define the empty dataframe as shown below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sImVhe06jiUi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_cars = pd.DataFrame(columns=[\"Price\", \"Title\", \"Posting Date\", \"URL\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9M1I48bjiUi",
        "outputId": "a9020e51-0b18-48c4-951e-718716284205"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>Title</th>\n",
              "      <th>Posting Date</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Price, Title, Posting Date, URL]\n",
              "Index: []"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#when executed, an empty DataFrame with the above 4 columns should be displayed\n",
        "df_cars"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jyF1GIBbjiUi"
      },
      "source": [
        "### Question 3 \n",
        "Write a function, open_page_and_return_soup, that takes two arguments: the link to the CL homepage to open and the category name. The function will then use this information to first extract the URL corresponding to that category on the homepage link provided. Needles to say, you should use the function you wrote above inside this function. Afterwards, the function should open the requested cars+trucks page, turn the page contents into a BeautifulSoup object, and return that BeautifulSoup object. \n",
        "\n",
        "Refer to the sample function call below. Pay attention to the data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCJ_nfwXjiUi"
      },
      "outputs": [],
      "source": [
        "def open_page_and_return_soup(cl_url, cl_label):\n",
        "    \"\"\" \n",
        "        gets soup of results page url based on craiglist url\n",
        "            \n",
        "        Args:\n",
        "            cl_url (string): craigslist location url\n",
        "            cl_label (string): category label\n",
        "            \n",
        "        Returns:\n",
        "            bs4.BeautifulSoup: soup of page results\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    url = get_url_from_cl(cl_url, cl_label)\n",
        "    if not url.startswith(\"https://\"):\n",
        "        # returns previous error string\n",
        "        return url\n",
        "    else:\n",
        "        # open page into soup\n",
        "        try:\n",
        "            html = urllib.urlopen(url)\n",
        "            soup = BeautifulSoup(html.read())\n",
        "            html.close()\n",
        "        except HTTPError:\n",
        "            return \"request failed\"\n",
        "        return soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQNU0c7DjiUi"
      },
      "outputs": [],
      "source": [
        "soup = open_page_and_return_soup(\"https://boston.craigslist.org\", \"cars+trucks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg4tKbXujiUj",
        "outputId": "515941d6-1cab-43f5-b08f-c13295d2977c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(soup)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mc2mVXA3jiUj"
      },
      "source": [
        "### Warning:\n",
        "If you make multiple requests, CL will eventually block you! That's why you are being asked to create the BeautifulSoup object here and avoid executing the URL request repeatedly. Once created, **soup** will enable you to do want you want. Refrain from making multiple requests to CL! They won't like it. Part of the assignment involves you not getting blocked. It's a reality of web scraping."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LsbuYvXCjiUj"
      },
      "source": [
        "### Question 4 \n",
        "Write a code snippet that extracts from your BeautifulSoup object the aforementioned 4 pieces of information about each post: **listing price of the car**, **the title of the post**, **when the listing was posted**, and **the link to the post**.\n",
        "\n",
        "For each listing, you should store these values in their corresponding columns of the df_cars DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC-3zHkJjiUj"
      },
      "outputs": [],
      "source": [
        "# array to append all results\n",
        "vehicles = []\n",
        "\n",
        "# get a list of the results (rows of df)\n",
        "vehicle_divs = soup.find_all(\"li\", class_=\"result-row\")\n",
        "\n",
        "# get properties we are looking for (columns of df)\n",
        "for vehicle in vehicle_divs:\n",
        "    list_price = vehicle.find(\"span\", \"result-price\").get_text().strip()\n",
        "    title = vehicle.find(\"a\", \"result-title\").get_text().strip()\n",
        "    list_date = vehicle.find(\"time\", \"result-date\").get_text().strip()\n",
        "    post_url = vehicle.find(\"a\", \"result-title\").get(\"href\").strip()\n",
        "    vehicle_dict = {\n",
        "        \"Price\": list_price,\n",
        "        \"Title\": title,\n",
        "        \"Posting Date\": list_date,\n",
        "        \"URL\": post_url\n",
        "    }\n",
        "    vehicles.append(vehicle_dict)\n",
        "\n",
        "# add all results to df at once\n",
        "df_cars = df_cars.append(vehicles)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YRUa8kdqjiUj"
      },
      "source": [
        "When executed, the following line should display your DataFrame as follows:\n",
        "* Note that the price values do not contain the $ sign.\n",
        "* Some of these fields may not be available for all listings. Your code should take that into account and leave the field value empty (\"\") when a field is not available for the listing.\n",
        "* Pay attention to the format of the posting date. There may be multiple formats in the HTML, but you should scrape the one displayed below.\n",
        "\n",
        "\n",
        "Refer to the sample output below. Note that by the time you scrape the page, new postings will have been made. Thus, your listings will differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKCvVjjdjiUj",
        "outputId": "915923b3-57ce-4c94-9137-2ecc4ee04e94",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>Title</th>\n",
              "      <th>Posting Date</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>$22,500</td>\n",
              "      <td>2013 Chevy Silverado 1500</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/nwb/cto/d/dracut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>$800</td>\n",
              "      <td>1995 Chevrolet C/K Pickup 1500 Regular Cab 4WD</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/bozema...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>$99</td>\n",
              "      <td>2020 Hyundai Accent SE - BAD CREDIT OK!</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/nwb/ctd/d/salem-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>$2,900</td>\n",
              "      <td>2004 Audi S4 Quattro AWD V-8</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/cto/d/pittsf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>$12,495</td>\n",
              "      <td>2015 Toyota Corolla LE</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/malden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>$5,995</td>\n",
              "      <td>2010 Hyundai Genesis v8 LOADED</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/east-d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>$21,000</td>\n",
              "      <td>2013 FX4, Loaded</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/nos/cto/d/danver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>$5,995</td>\n",
              "      <td>2004 Dodge Ram Pickup 2500 Laramie 4dr Quad Ca...</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/east-d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>$6,995</td>\n",
              "      <td>2007 Chevrolet Silverado 1500 LT1 4dr Extended...</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/east-d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>$4,995</td>\n",
              "      <td>2007 Nissan Murano AWD Leather LOADED</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/east-d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Price                                              Title Posting Date  \\\n",
              "0    $22,500                          2013 Chevy Silverado 1500       Sep 18   \n",
              "1       $800     1995 Chevrolet C/K Pickup 1500 Regular Cab 4WD       Sep 18   \n",
              "2        $99            2020 Hyundai Accent SE - BAD CREDIT OK!       Sep 18   \n",
              "3     $2,900                       2004 Audi S4 Quattro AWD V-8       Sep 18   \n",
              "4    $12,495                             2015 Toyota Corolla LE       Sep 18   \n",
              "..       ...                                                ...          ...   \n",
              "115   $5,995                     2010 Hyundai Genesis v8 LOADED       Sep 18   \n",
              "116  $21,000                                   2013 FX4, Loaded       Sep 18   \n",
              "117   $5,995  2004 Dodge Ram Pickup 2500 Laramie 4dr Quad Ca...       Sep 18   \n",
              "118   $6,995  2007 Chevrolet Silverado 1500 LT1 4dr Extended...       Sep 18   \n",
              "119   $4,995              2007 Nissan Murano AWD Leather LOADED       Sep 18   \n",
              "\n",
              "                                                   URL  \n",
              "0    https://boston.craigslist.org/nwb/cto/d/dracut...  \n",
              "1    https://boston.craigslist.org/gbs/ctd/d/bozema...  \n",
              "2    https://boston.craigslist.org/nwb/ctd/d/salem-...  \n",
              "3    https://boston.craigslist.org/gbs/cto/d/pittsf...  \n",
              "4    https://boston.craigslist.org/gbs/ctd/d/malden...  \n",
              "..                                                 ...  \n",
              "115  https://boston.craigslist.org/gbs/ctd/d/east-d...  \n",
              "116  https://boston.craigslist.org/nos/cto/d/danver...  \n",
              "117  https://boston.craigslist.org/gbs/ctd/d/east-d...  \n",
              "118  https://boston.craigslist.org/gbs/ctd/d/east-d...  \n",
              "119  https://boston.craigslist.org/gbs/ctd/d/east-d...  \n",
              "\n",
              "[120 rows x 4 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cars"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oOuyztzcjiUk"
      },
      "source": [
        "### Question 5 \n",
        "Being the pain in the neck he is, Tony would also like you to extract two other pieces of information from each listing's own page and add those to the dataframe. Specifically, for each car in the list, he would like you to retrieve the odometer value of the car as well as transmission information (whether it is manual, automatic, etc.). It turns out he cannot drive a stick shift. Both fields can only be accessed from each listing's own page, so the homepage isn't helpful here.\n",
        "\n",
        "To get started, pick a car listing and open the posting in your browser. On the right-hand side, you will see the attributes of the car in gray boxes. Odometer and transmission information, when available, is shown there too. Study the HTML code for that portion of the screen!\n",
        "\n",
        "\n",
        "* Your code snippet should look through the above BeautifulSoup object. \n",
        "    * **Hint:** Note that in Q4, you already got a list of the cars from the BeautifulSoup object, so you can use that here.\n",
        "* For each listing, go to the listing's page. Once you have accessed the page information, remember to close the connection! Otherwise, you may get blocked.\n",
        "* Retrieve the odometer and transmission information, if available, and store them\n",
        "    * If these fields are not available, use empty (\"\") placeholder values for these fields.\n",
        "* As we discussed, before moving on to the next car in the list of cars, you might find it useful to pause the program for 5-10 seconds (randomly determined). This will take some time, of course. However, CL is really snippy about people crawling their site. You will get blocked, otherwise, and you really cannot afford that if you want to finish the assignment on time. :)\n",
        "    * If you do get blocked, try another computer or Google Colab on a lab computer on campus\n",
        "    \n",
        "Once you have this information for all cars, update the df_cars dataframe with the odometer and transmission information as shown in the sample output below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm0Bx_hsjiUk"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTiXNg0pjiUk",
        "outputId": "c2a56db0-49b9-481f-d247-347a92b30aee",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching car 1 of 120...\n",
            "Fetching car 2 of 120...\n",
            "Fetching car 3 of 120...\n",
            "Fetching car 4 of 120...\n",
            "Fetching car 5 of 120...\n",
            "Fetching car 6 of 120...\n",
            "Fetching car 7 of 120...\n",
            "Fetching car 8 of 120...\n",
            "Fetching car 9 of 120...\n",
            "Fetching car 10 of 120...\n",
            "Fetching car 11 of 120...\n",
            "Fetching car 12 of 120...\n",
            "Fetching car 13 of 120...\n",
            "Fetching car 14 of 120...\n",
            "Fetching car 15 of 120...\n",
            "Fetching car 16 of 120...\n",
            "Fetching car 17 of 120...\n",
            "Fetching car 18 of 120...\n",
            "Fetching car 19 of 120...\n",
            "Fetching car 20 of 120...\n",
            "Fetching car 21 of 120...\n",
            "Fetching car 22 of 120...\n",
            "Fetching car 23 of 120...\n",
            "Fetching car 24 of 120...\n",
            "Fetching car 25 of 120...\n",
            "Fetching car 26 of 120...\n",
            "Fetching car 27 of 120...\n",
            "Fetching car 28 of 120...\n",
            "Fetching car 29 of 120...\n",
            "Fetching car 30 of 120...\n",
            "Fetching car 31 of 120...\n",
            "Fetching car 32 of 120...\n",
            "Fetching car 33 of 120...\n",
            "Fetching car 34 of 120...\n",
            "Fetching car 35 of 120...\n",
            "Fetching car 36 of 120...\n",
            "Fetching car 37 of 120...\n",
            "Fetching car 38 of 120...\n",
            "Fetching car 39 of 120...\n",
            "Fetching car 40 of 120...\n",
            "Fetching car 41 of 120...\n",
            "Fetching car 42 of 120...\n",
            "Fetching car 43 of 120...\n",
            "Fetching car 44 of 120...\n",
            "Fetching car 45 of 120...\n",
            "Fetching car 46 of 120...\n",
            "Fetching car 47 of 120...\n",
            "Fetching car 48 of 120...\n",
            "Fetching car 49 of 120...\n",
            "Fetching car 50 of 120...\n",
            "Fetching car 51 of 120...\n",
            "Fetching car 52 of 120...\n",
            "Fetching car 53 of 120...\n",
            "Fetching car 54 of 120...\n",
            "Fetching car 55 of 120...\n",
            "Fetching car 56 of 120...\n",
            "Fetching car 57 of 120...\n",
            "Fetching car 58 of 120...\n",
            "Fetching car 59 of 120...\n",
            "Fetching car 60 of 120...\n",
            "Fetching car 61 of 120...\n",
            "Fetching car 62 of 120...\n",
            "Fetching car 63 of 120...\n",
            "Fetching car 64 of 120...\n",
            "Fetching car 65 of 120...\n",
            "Fetching car 66 of 120...\n",
            "Fetching car 67 of 120...\n",
            "Fetching car 68 of 120...\n",
            "Fetching car 69 of 120...\n",
            "Fetching car 70 of 120...\n",
            "Fetching car 71 of 120...\n",
            "Fetching car 72 of 120...\n",
            "Fetching car 73 of 120...\n",
            "Fetching car 74 of 120...\n",
            "Fetching car 75 of 120...\n",
            "Fetching car 76 of 120...\n",
            "Fetching car 77 of 120...\n",
            "Fetching car 78 of 120...\n",
            "Fetching car 79 of 120...\n",
            "Fetching car 80 of 120...\n",
            "Fetching car 81 of 120...\n",
            "Fetching car 82 of 120...\n",
            "Fetching car 83 of 120...\n",
            "Fetching car 84 of 120...\n",
            "Fetching car 85 of 120...\n",
            "Fetching car 86 of 120...\n",
            "Fetching car 87 of 120...\n",
            "Fetching car 88 of 120...\n",
            "Fetching car 89 of 120...\n",
            "Fetching car 90 of 120...\n",
            "Fetching car 91 of 120...\n",
            "Fetching car 92 of 120...\n",
            "Fetching car 93 of 120...\n",
            "Fetching car 94 of 120...\n",
            "Fetching car 95 of 120...\n",
            "Fetching car 96 of 120...\n",
            "Fetching car 97 of 120...\n",
            "Fetching car 98 of 120...\n",
            "Fetching car 99 of 120...\n",
            "Fetching car 100 of 120...\n",
            "Fetching car 101 of 120...\n",
            "Fetching car 102 of 120...\n",
            "Fetching car 103 of 120...\n",
            "Fetching car 104 of 120...\n",
            "Fetching car 105 of 120...\n",
            "Fetching car 106 of 120...\n",
            "Fetching car 107 of 120...\n",
            "Fetching car 108 of 120...\n",
            "Fetching car 109 of 120...\n",
            "Fetching car 110 of 120...\n",
            "Fetching car 111 of 120...\n",
            "Fetching car 112 of 120...\n",
            "Fetching car 113 of 120...\n",
            "Fetching car 114 of 120...\n",
            "Fetching car 115 of 120...\n",
            "Fetching car 116 of 120...\n",
            "Fetching car 117 of 120...\n",
            "Fetching car 118 of 120...\n",
            "Fetching car 119 of 120...\n",
            "Fetching car 120 of 120...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# fixed number of listings to optimize np arrays\n",
        "num_list = df_cars.shape[0]\n",
        "\n",
        "# np arrays to store new column info\n",
        "# use '' as default\n",
        "odometers = np.zeros(num_list, np.dtype(\"U10\"))\n",
        "transmissions = np.zeros(num_list, np.dtype(\"U20\"))\n",
        "\n",
        "# get new column info for each listing\n",
        "for i in range(num_list):\n",
        "    \n",
        "    # provide indication for user that we are still scraping dataa\n",
        "    print(f\"Fetching car {i + 1} of {num_list}...\")\n",
        "    \n",
        "    # get row\n",
        "    vehicle_series = df_cars.iloc[i]\n",
        "    \n",
        "    # wait after first request so we don't get blocked\n",
        "    if i != 0:\n",
        "        time.sleep(random.randint(5,10))\n",
        "        \n",
        "    # retrieve page data in soup\n",
        "    html = urllib.urlopen(vehicle_series[\"URL\"])\n",
        "    soup = BeautifulSoup(html.read())\n",
        "    html.close()\n",
        "    \n",
        "    # get attributes on cl sidebar\n",
        "    attributes = soup.find_all(\"p\", \"attrgroup\")\n",
        "    for all_attr in attributes:\n",
        "        for attr in all_attr:\n",
        "            # find attributes with span\n",
        "            if attr.name == \"span\":\n",
        "                attr_text = attr.get_text().strip()\n",
        "                # check span text to see if it matches columns we need\n",
        "                # extract data if it fits our model, else continue loop\n",
        "                if attr_text.startswith(\"odometer:\"):\n",
        "                    try:\n",
        "                        odometers[i] = attr_text.split(\":\")[1].strip()\n",
        "                    except:\n",
        "                        continue\n",
        "                elif attr_text.startswith(\"transmission:\"):\n",
        "                    try:\n",
        "                        transmissions[i] = attr_text.split(\":\")[1].strip()\n",
        "                    except: continue\n",
        "\n",
        "# indicate we are done scraping data\n",
        "print(\"Done!\")\n",
        "\n",
        "# add new columns to df\n",
        "df_cars[\"Odometer\"] = odometers\n",
        "df_cars[\"Transmission\"] = transmissions\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B7VbLGFjiUk",
        "outputId": "f2c6d115-7c2c-4aaf-b487-6cf2511e242f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>Title</th>\n",
              "      <th>Posting Date</th>\n",
              "      <th>URL</th>\n",
              "      <th>Odometer</th>\n",
              "      <th>Transmission</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>$22,500</td>\n",
              "      <td>2013 Chevy Silverado 1500</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/nwb/cto/d/dracut...</td>\n",
              "      <td>94000</td>\n",
              "      <td>automatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>$800</td>\n",
              "      <td>1995 Chevrolet C/K Pickup 1500 Regular Cab 4WD</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/bozema...</td>\n",
              "      <td>127190</td>\n",
              "      <td>automatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>$99</td>\n",
              "      <td>2020 Hyundai Accent SE - BAD CREDIT OK!</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/nwb/ctd/d/salem-...</td>\n",
              "      <td>14216</td>\n",
              "      <td>automatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>$2,900</td>\n",
              "      <td>2004 Audi S4 Quattro AWD V-8</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/cto/d/pittsf...</td>\n",
              "      <td>140000</td>\n",
              "      <td>automatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>$12,495</td>\n",
              "      <td>2015 Toyota Corolla LE</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/malden...</td>\n",
              "      <td>127160</td>\n",
              "      <td>automatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>$5,995</td>\n",
              "      <td>2010 Hyundai Genesis v8 LOADED</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/east-d...</td>\n",
              "      <td>161531</td>\n",
              "      <td>automatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>$21,000</td>\n",
              "      <td>2013 FX4, Loaded</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/nos/cto/d/danver...</td>\n",
              "      <td>118000</td>\n",
              "      <td>automatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>$5,995</td>\n",
              "      <td>2004 Dodge Ram Pickup 2500 Laramie 4dr Quad Ca...</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/east-d...</td>\n",
              "      <td>188868</td>\n",
              "      <td>automatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>$6,995</td>\n",
              "      <td>2007 Chevrolet Silverado 1500 LT1 4dr Extended...</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/east-d...</td>\n",
              "      <td>140154</td>\n",
              "      <td>automatic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>$4,995</td>\n",
              "      <td>2007 Nissan Murano AWD Leather LOADED</td>\n",
              "      <td>Sep 18</td>\n",
              "      <td>https://boston.craigslist.org/gbs/ctd/d/east-d...</td>\n",
              "      <td>155480</td>\n",
              "      <td>other</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Price                                              Title Posting Date  \\\n",
              "0    $22,500                          2013 Chevy Silverado 1500       Sep 18   \n",
              "1       $800     1995 Chevrolet C/K Pickup 1500 Regular Cab 4WD       Sep 18   \n",
              "2        $99            2020 Hyundai Accent SE - BAD CREDIT OK!       Sep 18   \n",
              "3     $2,900                       2004 Audi S4 Quattro AWD V-8       Sep 18   \n",
              "4    $12,495                             2015 Toyota Corolla LE       Sep 18   \n",
              "..       ...                                                ...          ...   \n",
              "115   $5,995                     2010 Hyundai Genesis v8 LOADED       Sep 18   \n",
              "116  $21,000                                   2013 FX4, Loaded       Sep 18   \n",
              "117   $5,995  2004 Dodge Ram Pickup 2500 Laramie 4dr Quad Ca...       Sep 18   \n",
              "118   $6,995  2007 Chevrolet Silverado 1500 LT1 4dr Extended...       Sep 18   \n",
              "119   $4,995              2007 Nissan Murano AWD Leather LOADED       Sep 18   \n",
              "\n",
              "                                                   URL Odometer Transmission  \n",
              "0    https://boston.craigslist.org/nwb/cto/d/dracut...    94000    automatic  \n",
              "1    https://boston.craigslist.org/gbs/ctd/d/bozema...   127190    automatic  \n",
              "2    https://boston.craigslist.org/nwb/ctd/d/salem-...    14216    automatic  \n",
              "3    https://boston.craigslist.org/gbs/cto/d/pittsf...   140000    automatic  \n",
              "4    https://boston.craigslist.org/gbs/ctd/d/malden...   127160    automatic  \n",
              "..                                                 ...      ...          ...  \n",
              "115  https://boston.craigslist.org/gbs/ctd/d/east-d...   161531    automatic  \n",
              "116  https://boston.craigslist.org/nos/cto/d/danver...   118000    automatic  \n",
              "117  https://boston.craigslist.org/gbs/ctd/d/east-d...   188868    automatic  \n",
              "118  https://boston.craigslist.org/gbs/ctd/d/east-d...   140154    automatic  \n",
              "119  https://boston.craigslist.org/gbs/ctd/d/east-d...   155480        other  \n",
              "\n",
              "[120 rows x 6 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p75WCEXpjiUk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DS3000_HW2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
